{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98de4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanh/miniconda3/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from utils.inkml2img import convert_dir\n",
    "from tqdm.auto import tqdm\n",
    "from data.dataset import Im2LatexDataset\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from model.vit import ViT\n",
    "import lightning as L\n",
    "import wandb\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06148a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dims = [224, 640]\n",
    "model = ViT(img_dims, 16, n_embd=512)\n",
    "data = Im2LatexDataset(path_to_data=\"../data/\",\n",
    "                       tokenizer=\"../data/tokenizer.json\", img_dims=img_dims, batch_size=1, device=torch.device('mps'))\n",
    "batch = next(iter(data.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2f5ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[BOS]\\\\rho_{\\\\Delta\\\\Omega}(\\\\overline{\\\\sigma},\\\\mu)\\\\to\\\\rho_{\\\\infty}(\\\\overline{\\\\sigma},\\\\mu)=\\\\overline{n}[EOS][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(data.train))\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='../data/tokenizer.json')\n",
    "tokens = list(map(lambda x: x.data, batch[1]['input_ids'][0]))\n",
    "dec = [tokenizer.decode(tok) for tok in tokens]\n",
    "''.join([''.join(detok.split(' ')).replace('Ä ', ' ').strip() for detok in dec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2714bd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_tokenizer(equations, output, vocab_size):\n",
    "    from tokenizers import Tokenizer, pre_tokenizers\n",
    "    from tokenizers.models import BPE\n",
    "    from tokenizers.trainers import BpeTrainer\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"], vocab_size=vocab_size, show_progress=True)\n",
    "    tokenizer.train([equations], trainer)\n",
    "    tokenizer.save(path=output, pretty=False)\n",
    "    \n",
    "generate_tokenizer('../data/train_equations.txt', '../data/tokenizer.json', 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7004868e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

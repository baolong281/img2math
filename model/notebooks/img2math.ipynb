{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ad4f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from utils.inkml2img import convert_dir\n",
    "from tqdm.auto import tqdm\n",
    "from data.dataset import Im2LatexDataset\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from model.vit import ViT\n",
    "import lightning as L\n",
    "import wandb\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from model.decoder import Decoder, DecoderAttention, DecoderTransformerBlock\n",
    "from model.model import Img2MathModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79858ec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 10]) tensor([[[ 1.8602, -1.7757, -0.4856,  0.3035, -0.3928, -0.1712, -1.0683,\n",
      "           1.2496,  0.0723,  0.4081],\n",
      "         [ 1.8533, -1.7807, -0.4830,  0.3080, -0.4076, -0.1653, -1.0573,\n",
      "           1.2619,  0.0747,  0.3960],\n",
      "         [ 1.8644, -1.7697, -0.4828,  0.3050, -0.3896, -0.1743, -1.0882,\n",
      "           1.2317,  0.0846,  0.4189],\n",
      "         [ 1.8424, -1.7945, -0.4692,  0.3055, -0.3958, -0.1585, -1.0812,\n",
      "           1.2375,  0.0867,  0.4271],\n",
      "         [ 1.8504, -1.7846, -0.4864,  0.3131, -0.4047, -0.1716, -1.0642,\n",
      "           1.2451,  0.0875,  0.4154],\n",
      "         [ 1.8518, -1.7806, -0.4874,  0.3155, -0.4064, -0.1590, -1.0753,\n",
      "           1.2393,  0.0859,  0.4161],\n",
      "         [ 1.8586, -1.7793, -0.4924,  0.3128, -0.3979, -0.1742, -1.0630,\n",
      "           1.2432,  0.0820,  0.4101],\n",
      "         [ 1.8525, -1.7855, -0.4863,  0.3128, -0.4014, -0.1725, -1.0614,\n",
      "           1.2468,  0.0875,  0.4076],\n",
      "         [ 1.8557, -1.7789, -0.4900,  0.3150, -0.4027, -0.1569, -1.0750,\n",
      "           1.2391,  0.0849,  0.4088],\n",
      "         [ 1.8484, -1.7908, -0.4800,  0.3098, -0.4053, -0.1573, -1.0725,\n",
      "           1.2373,  0.0973,  0.4129],\n",
      "         [ 1.8573, -1.7816, -0.4909,  0.3122, -0.3996, -0.1680, -1.0623,\n",
      "           1.2430,  0.0766,  0.4132],\n",
      "         [ 1.8567, -1.7757, -0.4807,  0.3083, -0.3998, -0.1595, -1.0728,\n",
      "           1.2526,  0.0660,  0.4049],\n",
      "         [ 1.8619, -1.7710, -0.5003,  0.3185, -0.4079, -0.1557, -1.0663,\n",
      "           1.2436,  0.0695,  0.4076],\n",
      "         [ 1.8480, -1.7939, -0.4787,  0.3074, -0.3908, -0.1547, -1.0716,\n",
      "           1.2425,  0.0824,  0.4094],\n",
      "         [ 1.8518, -1.7824, -0.4898,  0.3098, -0.4081, -0.1661, -1.0606,\n",
      "           1.2498,  0.0830,  0.4124],\n",
      "         [ 1.8627, -1.7681, -0.5016,  0.3143, -0.3990, -0.1665, -1.0671,\n",
      "           1.2490,  0.0737,  0.4027],\n",
      "         [ 1.8557, -1.8113, -0.4771,  0.2925, -0.3902, -0.1375, -1.0439,\n",
      "           1.2363,  0.0650,  0.4105],\n",
      "         [ 1.8479, -1.8156, -0.4702,  0.2923, -0.3964, -0.1377, -1.0449,\n",
      "           1.2409,  0.0726,  0.4112],\n",
      "         [ 1.8569, -1.8018, -0.4826,  0.3027, -0.4032, -0.1238, -1.0525,\n",
      "           1.2349,  0.0614,  0.4080],\n",
      "         [ 1.8512, -1.8072, -0.5132,  0.3159, -0.4173, -0.1237, -1.0314,\n",
      "           1.2349,  0.0950,  0.3958],\n",
      "         [ 1.8506, -1.8104, -0.5072,  0.3073, -0.4085, -0.1338, -1.0394,\n",
      "           1.2277,  0.1126,  0.4012],\n",
      "         [ 1.8564, -1.8038, -0.4988,  0.3095, -0.3988, -0.1254, -1.0602,\n",
      "           1.2201,  0.1050,  0.3959],\n",
      "         [ 1.8567, -1.7979, -0.5041,  0.3083, -0.4059, -0.1280, -1.0507,\n",
      "           1.2345,  0.0965,  0.3906],\n",
      "         [ 1.8448, -1.8145, -0.5036,  0.3038, -0.4094, -0.1225, -1.0463,\n",
      "           1.2279,  0.1221,  0.3978],\n",
      "         [ 1.8621, -1.7896, -0.5127,  0.3132, -0.4032, -0.1239, -1.0643,\n",
      "           1.2208,  0.1006,  0.3970],\n",
      "         [ 1.8538, -1.8138, -0.4965,  0.2900, -0.3685, -0.0987, -1.0717,\n",
      "           1.2194,  0.0953,  0.3907],\n",
      "         [ 1.8365, -1.8556, -0.4174,  0.2874, -0.3612, -0.0979, -1.0638,\n",
      "           1.2232,  0.0533,  0.3954],\n",
      "         [ 1.8492, -1.8407, -0.4450,  0.2979, -0.3750, -0.1207, -1.0449,\n",
      "           1.2185,  0.0462,  0.4146],\n",
      "         [ 1.8495, -1.8380, -0.4478,  0.2976, -0.3764, -0.1221, -1.0453,\n",
      "           1.2205,  0.0489,  0.4132],\n",
      "         [ 1.8525, -1.8255, -0.4549,  0.3014, -0.3746, -0.1145, -1.0626,\n",
      "           1.2209,  0.0545,  0.4027],\n",
      "         [ 1.8513, -1.8269, -0.4524,  0.3033, -0.3841, -0.1148, -1.0481,\n",
      "           1.2312,  0.0382,  0.4023],\n",
      "         [ 1.8488, -1.8292, -0.4543,  0.2920, -0.3656, -0.1071, -1.0695,\n",
      "           1.2221,  0.0668,  0.3962]]], device='mps:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanh/miniconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/core/module.py:407: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(7.6332, device='mps:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "img_dims = [256, 256]\n",
    "data = Im2LatexDataset(path_to_data=\"../data/\",\n",
    "                       tokenizer=\"../data/tokenizer.json\", block_size=32, img_dims=img_dims, batch_size=1, device=device)\n",
    "batch = next(iter(data.train))\n",
    "img, label = batch\n",
    "vocab_size = len(data.tokenizer.get_vocab())\n",
    "#model = Img2MathModel.load_from_checkpoint('../train/img2math/362edsd8/checkpoints/epoch=9-step=65180.ckpt', map_location=device)\n",
    "model = Img2MathModel(10, 32, 2160, 0, img_dims, 16, num_heads=2).to(device)\n",
    "model.training_step(batch, 1)\n",
    "#model.eval()\n",
    "#logits, _ = model(img)\n",
    "#torch.argmax(logits)\n",
    "#val_imgs = img\n",
    "#model.training_step(batch, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa7a840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 #preds = [model.generate(img) for img in val_imgs]</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span style=\"color: #808000; text-decoration-color: #808000\">'../data/tokenizer.json'</span>, padding_sid    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 8 seq = model.generate(val_imgs[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>])                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span>decode(tokenizer, seq)                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'val_imgs'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m8\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m#preds = [model.generate(img) for img in val_imgs]\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0mtokenizer = PreTrainedTokenizerFast(tokenizer_file=\u001b[33m'\u001b[0m\u001b[33m../data/tokenizer.json\u001b[0m\u001b[33m'\u001b[0m, padding_sid    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 8 seq = model.generate(val_imgs[\u001b[94m0\u001b[0m])                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0mdecode(tokenizer, seq)                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'val_imgs'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode(tokenizer, sequence):\n",
    "    dec = [tokenizer.decode(tok) for tok in sequence]\n",
    "    return ''.join([detok.replace('Ġ', ' ') for detok in dec])\n",
    "\n",
    "#preds = [model.generate(img) for img in val_imgs]\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='../data/tokenizer.json', padding_side='left')\n",
    "\n",
    "seq = model.generate(val_imgs[0])\n",
    "decode(tokenizer, seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca529c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbaolong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230706_213920-0jvbqwtx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/baolong/img2math/runs/0jvbqwtx' target=\"_blank\">zesty-eon-38</a></strong> to <a href='https://wandb.ai/baolong/img2math' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/baolong/img2math' target=\"_blank\">https://wandb.ai/baolong/img2math</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/baolong/img2math/runs/0jvbqwtx' target=\"_blank\">https://wandb.ai/baolong/img2math/runs/0jvbqwtx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | ViT     | 8.9 M \n",
      "1 | decoder | Decoder | 14.9 M\n",
      "------------------------------------\n",
      "23.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.9 M    Total params\n",
      "95.443    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanh/miniconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/dylanh/miniconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb23cd01263e4904ac33b2a3a0e61203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563ac08e8d4f4e6295e0c9219f297db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.229 MB of 0.229 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>global_step</td><td>▁█</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val/loss_epoch</td><td>▁</td></tr><tr><td>val/loss_step</td><td>▂▄▃▃▃▂▃▃▆▂▄▂▂▂▃▃█▁▂▂▁▃▄▃▂▆▂▂▁▃▃▂▄▃▃▂▇▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>global_step</td><td>10</td></tr><tr><td>train/loss</td><td>0.00864</td></tr><tr><td>trainer/global_step</td><td>9</td></tr><tr><td>val/loss_epoch</td><td>0.44301</td></tr><tr><td>val/loss_step</td><td>0.3337</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-eon-38</strong> at: <a href='https://wandb.ai/baolong/img2math/runs/0jvbqwtx' target=\"_blank\">https://wandb.ai/baolong/img2math/runs/0jvbqwtx</a><br/>Synced 6 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230706_213920-0jvbqwtx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TOKENIZER_FILE = '../data/tokenizer.json'\n",
    "\n",
    "def decode(tokenizer, sequence):\n",
    "    dec = [tokenizer.decode(tok) for tok in sequence]\n",
    "    return ''.join([detok.replace('Ġ', ' ') for detok in dec])\n",
    "\n",
    "class ImagePredictionLogger(L.Callback):\n",
    "    \n",
    "    def __init__(self, val_samples, num_samples=10):\n",
    "        super().__init__()\n",
    "        self.tokenizer = PreTrainedTokenizerFast(tokenizer_file=TOKENIZER_FILE, padding_side='left')\n",
    "        self.val_imgs, self.val_labels = val_samples\n",
    "        self.val_imgs = self.val_imgs[:num_samples]\n",
    "        self.val_labels = [decode(self.tokenizer, label) for label in self.val_labels['input_ids'][:num_samples]]\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "\n",
    "        pred_tokens = [pl_module.generate(x) for x in val_imgs]\n",
    "        preds = [decode(self.tokenizer, pred) for pred in pred_tokens]\n",
    "\n",
    "        trainer.logger.experiment.log({\n",
    "            \"examples\": [wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\")\n",
    "                         for x, pred, y in zip(val_imgs, preds, self.val_labels)],\n",
    "            \"global_step\": trainer.global_step\n",
    "        })\n",
    "\n",
    "logger = WandbLogger(project='img2math')\n",
    "\n",
    "trainer = L.Trainer(limit_train_batches=10, max_epochs=1, log_every_n_steps=10, deterministic=True,\n",
    "                    logger=logger, accelerator='mps', callbacks=[ImagePredictionLogger(batch)])\n",
    "\n",
    "trainer.fit(model, data.train, data.test)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f7744de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem usage: 1310720\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n",
      "mem usage: 1835008\n",
      "mem usage: 1572864\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "a = torch.randn(256, 512).to(device)\n",
    "\n",
    "for i in range(100):\n",
    "    b = torch.randn(256, 512).to(device)\n",
    "    c = a @ b.T\n",
    "    print(f\"mem usage: {torch._C._mps_currentAllocatedMemory()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e539a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

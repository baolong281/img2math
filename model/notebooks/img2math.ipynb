{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ad4f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from utils.inkml2img import convert_dir\n",
    "from tqdm.auto import tqdm\n",
    "from data.dataset import Im2LatexDataset\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from model.vit import ViT\n",
    "import lightning as L\n",
    "import wandb\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from model.decoder import Decoder, DecoderAttention, DecoderTransformerBlock\n",
    "from model.model import Img2MathModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79858ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img shape: torch.Size([16, 1, 256, 256])\n",
      "shapes: torch.Size([16, 256, 512]), torch.Size([16, 256, 512]), torch.Size([16, 256, 512])\n",
      "shapes: torch.Size([16, 256, 512]), torch.Size([16, 256, 512]), torch.Size([16, 256, 512])\n",
      "shapes: torch.Size([16, 256, 512]), torch.Size([16, 256, 512]), torch.Size([16, 256, 512])\n",
      "shapes: torch.Size([16, 256, 512]), torch.Size([16, 256, 512]), torch.Size([16, 256, 512])\n",
      "shapes: torch.Size([16, 256, 512]), torch.Size([16, 256, 512]), torch.Size([16, 256, 512])\n",
      "shapes: torch.Size([16, 256, 512]), torch.Size([16, 256, 512]), torch.Size([16, 256, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.8865,  0.3352, -1.0838,  ...,  0.0707,  0.2520, -0.1882],\n",
       "          [-0.7501, -0.3466,  0.5451,  ..., -0.4106,  0.4517,  0.2885],\n",
       "          [-0.3828, -0.4525,  0.4197,  ..., -1.0458,  0.5796,  0.5847],\n",
       "          ...,\n",
       "          [-0.4034, -0.0629,  0.3408,  ...,  0.4006, -0.9175,  0.0066],\n",
       "          [-1.2436, -0.4131,  0.8931,  ..., -0.0599, -0.3454, -0.2364],\n",
       "          [-0.1735,  0.1070,  0.6850,  ..., -0.0255, -0.6688, -0.3128]],\n",
       " \n",
       "         [[-1.1732, -0.1762, -0.1384,  ...,  0.7157, -0.5097, -0.7380],\n",
       "          [-0.4457,  0.2216, -0.0070,  ...,  1.1994, -0.2420,  0.1500],\n",
       "          [ 0.0350,  0.4971,  0.2238,  ...,  0.2921, -0.7589, -0.3196],\n",
       "          ...,\n",
       "          [-0.2415, -0.7701,  0.6804,  ..., -0.2155, -1.1124, -0.4515],\n",
       "          [-0.3333,  0.1943,  0.2739,  ..., -0.0220, -0.6578, -0.2252],\n",
       "          [-0.1498, -0.2350,  0.0071,  ..., -0.4899, -0.3448, -0.5391]],\n",
       " \n",
       "         [[-0.6463,  0.4531,  0.1762,  ...,  0.0147, -0.0365,  0.7357],\n",
       "          [-1.5570, -0.4283,  0.3068,  ...,  0.4213, -0.4568, -0.2001],\n",
       "          [-0.8890,  0.4091, -0.3230,  ...,  0.2948,  0.4054,  0.8831],\n",
       "          ...,\n",
       "          [-1.2657,  0.3621,  0.1126,  ...,  0.7729,  0.5659,  0.0539],\n",
       "          [-0.5041,  0.1740, -0.1507,  ...,  0.1427, -1.2246, -0.0393],\n",
       "          [-0.7478, -0.4978, -0.1718,  ...,  0.7107,  0.0263,  0.0419]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.2882, -1.1372,  0.2412,  ...,  0.1039, -0.6112,  0.2762],\n",
       "          [-0.5669, -1.4616,  0.0514,  ..., -0.2145,  0.2208,  0.1595],\n",
       "          [-0.7942,  0.2685,  0.3004,  ...,  0.2189, -0.6640, -0.9108],\n",
       "          ...,\n",
       "          [-0.4407,  0.0433, -0.0320,  ...,  0.2280, -0.4839,  0.6242],\n",
       "          [-0.7204,  0.1427, -0.1458,  ...,  0.1156,  0.0039, -0.1528],\n",
       "          [-1.0526, -0.5109, -0.1559,  ..., -0.0678, -0.5748,  0.2362]],\n",
       " \n",
       "         [[ 0.0542,  0.9047, -0.3340,  ...,  0.1912, -1.3251,  0.1286],\n",
       "          [-0.5752,  0.9717, -0.4359,  ...,  1.1987, -1.0354,  0.3437],\n",
       "          [-1.4648,  0.5584,  0.4264,  ...,  0.5752, -0.4211, -0.3318],\n",
       "          ...,\n",
       "          [-0.5376,  0.1526, -0.3653,  ...,  0.3390,  0.2375, -0.2844],\n",
       "          [-0.5064,  0.8194, -0.5887,  ...,  0.5954, -0.6382, -0.3319],\n",
       "          [-0.5054, -0.1297,  0.1856,  ..., -0.0201, -0.6196,  0.7358]],\n",
       " \n",
       "         [[-0.3823,  0.6711,  0.1234,  ...,  0.1674, -1.5149, -1.2577],\n",
       "          [-0.5367, -0.0758, -0.1179,  ...,  1.1600, -0.9827, -0.5924],\n",
       "          [-1.2048,  0.3986,  0.0618,  ...,  0.2344, -0.4788, -1.0089],\n",
       "          ...,\n",
       "          [-0.6471, -0.2645,  0.6010,  ...,  0.1909,  0.0201, -0.0814],\n",
       "          [-1.5606, -0.9894,  0.0411,  ...,  0.2535, -1.4133,  0.0860],\n",
       "          [-0.6829,  0.6259,  0.8945,  ..., -0.1049, -0.3851,  0.0055]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor(8.3174, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dims = [256, 256]\n",
    "data = Im2LatexDataset(path_to_data=\"../data/\",\n",
    "                       tokenizer=\"../data/tokenizer.json\", img_dims=img_dims, batch_size=16, device=torch.device('mps'))\n",
    "img, label = next(iter(data.train))\n",
    "vocab_size = len(data.tokenizer.get_vocab())\n",
    "enc_output = torch.randn(1, 512, 512)\n",
    "model = Img2MathModel(512, 256, vocab_size, .75, img_dims, 16)\n",
    "model(img, input_seq=label['input_ids'], trg_seq=label['input_ids'], mask=label['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "428c1872-bdeb-455b-87e1-31e916774bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5057aed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1854, -1.0171,  1.1940,  ..., -0.5155, -0.2047, -0.7936],\n",
       "          [-0.1163,  0.2031,  1.0163,  ..., -0.7860,  0.8499, -0.9828],\n",
       "          [-0.1543,  0.6852,  0.7890,  ...,  1.0069,  1.0043, -0.1216],\n",
       "          ...,\n",
       "          [ 0.4720, -0.0263,  0.6055,  ...,  0.2206, -0.2633, -0.3533],\n",
       "          [-0.2364, -0.2472,  0.4905,  ...,  0.2950,  0.4586, -0.2331],\n",
       "          [-0.2259,  1.5125,  1.0990,  ...,  0.0903,  0.7188, -0.4231]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor(7.9361, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(data.train))\n",
    "img, labels = batch\n",
    "trg_seq, input_mask = labels['input_ids'], labels['attention_mask']\n",
    "zero = torch.zeros(1, 1)\n",
    "input_seq = torch.cat((zero, trg_seq), dim=-1)\n",
    "input_seq = input_seq[:, :-1].int()\n",
    "input_mask = torch.cat((zero, input_mask), dim=-1)\n",
    "input_mask= input_mask[:, :-1].int()\n",
    "model(img[0], input_seq=input_seq, trg_seq=trg_seq, mask=input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c1b4f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230630_172710-5wt9ygtx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/baolong/img2math/runs/5wt9ygtx' target=\"_blank\">fresh-wave-11</a></strong> to <a href='https://wandb.ai/baolong/img2math' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/baolong/img2math' target=\"_blank\">https://wandb.ai/baolong/img2math</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/baolong/img2math/runs/5wt9ygtx' target=\"_blank\">https://wandb.ai/baolong/img2math/runs/5wt9ygtx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | ViT     | 8.9 M \n",
      "1 | decoder | Decoder | 14.9 M\n",
      "------------------------------------\n",
      "23.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.9 M    Total params\n",
      "95.443    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanh/miniconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/dylanh/miniconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c0a2004b3848749978d4883753eaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanh/miniconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ccee9eaa4f484aa5e773bc3a9eb88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val/loss</td><td>▅▁▄▃▆▇▅▆▆▅▄▅▃▃▂▂▂▅▄▅▂▃▄▇▃▆▅▃█▃▇▄▁▄▇▅▅█▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>trainer/global_step</td><td>5039</td></tr><tr><td>val/loss</td><td>1.25875</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-wave-11</strong> at: <a href='https://wandb.ai/baolong/img2math/runs/5wt9ygtx' target=\"_blank\">https://wandb.ai/baolong/img2math/runs/5wt9ygtx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230630_172710-5wt9ygtx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = WandbLogger(project='img2math')\n",
    "\n",
    "trainer = L.Trainer(limit_train_batches=10000, max_epochs=1, log_every_n_steps=20, deterministic=True,\n",
    "                    logger=logger, accelerator='mps')\n",
    "\n",
    "trainer.fit(model, data.train, data.test)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7744de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e539a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanh/miniconda3/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from utils.inkml2img import convert_dir\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from data.dataset import Im2LatexDataset\n",
    "from model.vit import ViT\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing the module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dims = [224, 640]\n",
    "data = Im2LatexDataset(path_to_data=\"../data/\",\n",
    "                       tokenizer=\"../data/tokenizer.json\", img_dims=img_dims, classification=True)\n",
    "imgs, labels = next(iter(data.test))\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(img_dims, 16, n_embd=512, encoder=False, output_classes=10)\n",
    "model(imgs[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_num_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"gpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"../data/\"\n",
    "PATH_TO_HANDWRITTEN = PATH_TO_DATA + \"handwritten/\"\n",
    "# train_handwritten_df = convert_dir(PATH_TO_HANDWRITTEN + 'train', PATH_TO_HANDWRITTEN + \"train\")\n",
    "# train_handwritten_df.to_csv(PATH_TO_DATA + 'train_handwritten.csv', index=False)\n",
    "# val_handwritten_df = convert_dir(PATH_TO_HANDWRITTEN + 'test', PATH_TO_HANDWRITTEN + \"test\")\n",
    "# val_handwritten_df.to_csv(PATH_TO_DATA + 'val_handwritten.csv', index=False)\n",
    "\n",
    "train_handwritten_df = pd.read_csv(PATH_TO_DATA + \"train_handwritten.csv\")\n",
    "val_handwritten_df = pd.read_csv(PATH_TO_DATA + \"val_handwritten.csv\")\n",
    "val_handwritten_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_path(path):\n",
    "    return PATH_TO_DATA + \"images/\" + path\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(PATH_TO_DATA + \"im2latex_train.csv\")\n",
    "val_df = pd.read_csv(PATH_TO_DATA + \"im2latex_validate.csv\")\n",
    "test_df = pd.read_csv(PATH_TO_DATA + \"im2latex_test.csv\")\n",
    "\n",
    "\n",
    "dataframes = [train_df, val_df, test_df]\n",
    "\n",
    "for df in dataframes:\n",
    "    df[\"image\"] = df[\"image\"].map(lambda x: fix_path(x))\n",
    "\n",
    "print(f\"train len before {len(train_df)}\")\n",
    "\n",
    "train_df = pd.concat([train_df, train_handwritten_df])\n",
    "val_df = pd.concat([val_df, val_handwritten_df])\n",
    "\n",
    "print(f\"train length after {len(train_df)}\")\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_equations():\n",
    "    train_equations = train_df[\"formula\"]\n",
    "    with open(\"../data/train_equations.txt\", \"w\") as f:\n",
    "        for value in train_equations:\n",
    "            f.write(str(value) + \"\\n\")\n",
    "\n",
    "\n",
    "# get_train_equations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def generate_tokenizer(equations, output, vocab_size):\n",
    "    from tokenizers import Tokenizer, pre_tokenizers\n",
    "    from tokenizers.models import BPE\n",
    "    from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    trainer = BpeTrainer(\n",
    "        special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"],\n",
    "        vocab_size=vocab_size,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    tokenizer.train([equations], trainer)\n",
    "    tokenizer.save(path=output, pretty=False)\n",
    "\n",
    "\n",
    "# generate_tokenizer('../data/train_equations.txt' ,'../data/tokenizer.json', 8000)\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"../data/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"boobs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDataset(Dataset):\n",
    "    def __init__(self, image_paths, formulas, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.formulas = formulas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths.iloc[index]\n",
    "        image = Image.open(image_path)\n",
    "        formula = self.formulas.iloc[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, formula\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 600)),  # Resize to a specific size\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "# -------------------------------------------------\n",
    "BATCH_SIZE = 32\n",
    "# -------------------------------------------------\n",
    "\n",
    "train_dataset = ImagesDataset(\n",
    "    train_df[\"image\"], train_df[\"formula\"], transform=transform\n",
    ")\n",
    "val_dataset = ImagesDataset(\n",
    "    val_df[\"image\"], val_df[\"formula\"], transform=transform)\n",
    "test_dataset = ImagesDataset(\n",
    "    test_df[\"image\"], test_df[\"formula\"], transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, label = next(iter(train_dataloader))\n",
    "\n",
    "plt.imshow(im[0][0], cmap=\"gray\")\n",
    "label[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## writing module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer block with skip connections and layernorm\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, num_heads, dropout=0.20):\n",
    "        super().__init__()\n",
    "        self.embed_dim = n_embd\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.ln_1 = torch.nn.LayerNorm(n_embd)\n",
    "        self.attention = SelfAttention(n_embd, num_heads, dropout)\n",
    "        self.ln_2 = torch.nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.head(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, channels=1, embed_dim=512):\n",
    "        \"\"\"\n",
    "        img size: image shape\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        h, w = img_size\n",
    "        assert (\n",
    "            w % patch_size == 0 and h % patch_size == 0\n",
    "        ), \"image not divisable by patch size\"\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (h // patch_size) * (w // patch_size)\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            channels,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: batch of images (B, H, W)\n",
    "        return: (B, num_patches, num_embeddings)\n",
    "        \"\"\"\n",
    "        x = self.projection(x)  # (B, n_embd, n_patches / 2, n_patches / 2)\n",
    "        x = x.flatten(-2)  # (B, n_embd, n_patches)\n",
    "        x = x.transpose(-2, -1)  # (B, n_patches, n_embd)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_embd, n_heads=8, bias=False, attn_dropout=0.20, proj_dropout=0.20\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_heads == 0, \"n_embd not divisible by num heads\"\n",
    "        self.n_heads = n_heads\n",
    "        self.n_embd = n_embd\n",
    "        self.head_dim = n_embd // n_heads\n",
    "        self.dk = self.head_dim**-0.5  # sqrt dk for scaling\n",
    "\n",
    "        self.kqv = nn.Linear(n_embd, n_embd * 3, bias=bias)\n",
    "        self.projection = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.proj_dropout = nn.Dropout(proj_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, n_embd)\n",
    "        returns: (B, T, n_embd)\n",
    "        \"\"\"\n",
    "\n",
    "        B, T, C, = x.shape  # batch size, num tokens, n_embd\n",
    "        assert C == self.n_embd, \"input size does not equal n_embd\"\n",
    "\n",
    "        kqv = self.kqv(x)  # (B, T, n_embd*3)\n",
    "        kqv = kqv.reshape(B, T, 3, self.n_heads, self.head_dim)\n",
    "        kqv = kqv.permute(2, 0, 3, 1, 4)  # (3, B, n_heads, T, head_dim)\n",
    "        k, q, v = kqv  # (B, n_heads, T, head_dim)\n",
    "\n",
    "        attention = (\n",
    "            q @ k.transpose(-1, -2)\n",
    "        ) * self.dk  # (B, n_heads, T, head_dim) @ (B, n_heads, head_dim, T) -> (B, n_heads, T, T)\n",
    "        attention = attention.softmax(dim=-1)  # (B, n_heads, T, T)\n",
    "        attention = self.attn_dropout(attention)\n",
    "        aggregated_attention = (\n",
    "            attention @ v\n",
    "        )  # (B, n_heads, T, T) @ (B, n_heads, T, head_dim) -> (B, n_heads, T, head_dim)\n",
    "        print(aggregated_attention.shape)\n",
    "        x = aggregated_attention.transpose(1, 2)  # (B, T, n_heads, C)\n",
    "        x = x.flatten(2)  # (B, T, C)\n",
    "        x = self.projection(x)  # (B, T, C)\n",
    "        x = self.proj_dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "x = torch.ones(1, 1, 64, 64)\n",
    "patch = PatchEmbeddings([64, 64], 16)\n",
    "# attention = SelfAttention(512)\n",
    "x = patch(x)\n",
    "# att = attention(x)\n",
    "block = TransformerBlock(512, 8)\n",
    "block(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
